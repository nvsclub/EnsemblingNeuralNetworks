{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lib.models import BaseLearnerRegression, cyclic_cosine_annealing_lr\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from lib.utils import check_dir\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 10\n",
    "K_SPLITS = 3\n",
    "N_ESTIMATORS = 25\n",
    "ANNEALING = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSamplingEnsemble():\n",
    "    def __init__(self, max_samples=1.0, max_features=1.0, bootstrap=False, snapshot=False, stacking=False):\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.learners = []\n",
    "        self.learners_features = []\n",
    "        self.snapshot = snapshot\n",
    "        self.snapshot_learners = []\n",
    "        self.ncl_lambda = None\n",
    "        self.stacking = stacking\n",
    "\n",
    "    def add_learner(self, n_learners, input_size, hidden_sizes, activation, loss_function, dropout_rates=None, lr=0.001, annealing=None, ncl_lambda=None):\n",
    "        self.ncl_lambda = ncl_lambda\n",
    "        for i in range(n_learners):\n",
    "            self.learners.append(BaseLearnerRegression(int(input_size * self.max_features), hidden_sizes, activation, loss_function, dropout_rates, lr, annealing, ncl_lambda))\n",
    "\n",
    "    def setup_data_loaders(self, df_train_set):\n",
    "        for learner in self.learners:\n",
    "            _df_train_set = df_train_set.copy()\n",
    "            # Sampling features\n",
    "            if self.max_features < 1.0:\n",
    "                n_selected_features = int(len(_df_train_set.drop('target', axis=1).columns) * self.max_features)\n",
    "                _df_train_set = _df_train_set.drop('target', axis=1).sample(n_selected_features, axis=1)\n",
    "                _df_train_set['target'] = df_train_set['target']\n",
    "\n",
    "            # Sampling samples\n",
    "            _df_train_set = _df_train_set.sample(frac=self.max_samples, replace=self.bootstrap)\n",
    "            \n",
    "            # Setup data loaders        \n",
    "            learner.setup_data_loaders(_df_train_set)\n",
    "            self.learners_features.append(_df_train_set.drop(columns=['target']).columns)\n",
    "\n",
    "    def train(self, epochs):\n",
    "        if self.snapshot:\n",
    "            for learner in self.learners:\n",
    "                self.snapshot_learners.append([])\n",
    "        \n",
    "        if not self.snapshot:\n",
    "            if epochs < ANNEALING:\n",
    "                print(f'WARNING: Epochs ({epochs}) is less than annealing ({ANNEALING}) for a non-snapshot ensemble.')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Calculating consensus for NCL\n",
    "            consensus = None\n",
    "            if self.ncl_lambda != None:\n",
    "                predictions = []\n",
    "                for i, learner in enumerate(self.learners):\n",
    "                    predictions.append(learner.predict(learner.train_data_x))\n",
    "                consensus = np.array(predictions).mean(axis=0)\n",
    "\n",
    "            # Training\n",
    "            for i, learner in enumerate(self.learners):\n",
    "                # Adjusting learning rate\n",
    "                lr_update = cyclic_cosine_annealing_lr(learner.lr, ANNEALING, 0, epoch)\n",
    "\n",
    "                # Training for one epoch\n",
    "                learner.train(1, consensus=consensus, lr_update=lr_update)\n",
    "\n",
    "                # Saving snapshot\n",
    "                if self.snapshot:\n",
    "                    if ((epoch % ANNEALING) == (ANNEALING - 1)) and (epoch != 0) and (epoch != (epochs - 1)):\n",
    "                        self.snapshot_learners[i].append(copy.deepcopy(learner))\n",
    "        \n",
    "        if self.snapshot:\n",
    "            self.snapshot_learners[i].append(copy.deepcopy(learner))\n",
    "\n",
    "        if self.stacking:\n",
    "            base_models = self.learners\n",
    "            if self.snapshot:\n",
    "                base_models = []\n",
    "                for i in range(len(self.snapshot_learners)):\n",
    "                    base_models += self.snapshot_learners[i]\n",
    "            self.stacking_model = BaseLearnerRegression(input_size=len(base_models), hidden_sizes=[len(base_models)], activation='relu', loss_function=torch.nn.MSELoss(), lr=0.02)\n",
    "\n",
    "            predictions = []\n",
    "            for i, learner in enumerate(base_models):\n",
    "                predictions.append(learner.predict(learner.train_data_x))\n",
    "\n",
    "            df_predictions = pd.DataFrame(predictions).T\n",
    "            df_predictions['target'] = learner.train_data_y.numpy()\n",
    "            self.stacking_model.setup_data_loaders(df_predictions)\n",
    "\n",
    "            self.stacking_model.train(50)\n",
    "\n",
    "    def predict(self, df_test_set):\n",
    "        # Predict class labels\n",
    "        predictions = []\n",
    "        for i in range(len(self.learners)):\n",
    "            if self.snapshot:\n",
    "                predictions = []\n",
    "                for snapshot_learner in self.snapshot_learners[i]:\n",
    "                    predictions.append(snapshot_learner.predict(df_test_set[self.learners_features[i]]))\n",
    "                #predictions.append(np.array(snapshot_predictions).mean(axis=0))\n",
    "            else:\n",
    "                predictions.append(self.learners[i].predict(df_test_set[self.learners_features[i]]))\n",
    "        \n",
    "        df_predictions = pd.DataFrame(predictions, index=[f'base_learner_{i}' for i in range(len(predictions))]).T\n",
    "\n",
    "        # Stacking\n",
    "        if self.stacking:\n",
    "            stacked_predictions = self.stacking_model.predict(df_predictions)\n",
    "            return stacked_predictions, df_predictions\n",
    "        \n",
    "        mean_prediction = np.array(predictions).mean(axis=0)\n",
    "        return mean_prediction, df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m0 256 16 relu 0.020237275398132\n",
      "m1 16 32 tanh 0.0692863240945767\n",
      "m2 128 64 relu 0.0303691484592233\n",
      "m3 32 32 tanh 0.0948870733809866\n"
     ]
    }
   ],
   "source": [
    "df_parameters = pd.read_csv('results/parameter_search_results.csv')\n",
    "parameters = df_parameters[df_parameters.value == df_parameters.value.min()].iloc[0]\n",
    "for i in range(4):\n",
    "    print(f'm{i}', parameters['params_m' + str(i) + '_n_units_l1'], parameters['params_m' + str(i) + '_n_units_l2'], parameters['params_m' + str(i) + '_activation'], parameters['params_m' + str(i) + '_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>fold</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/geographical_origin_of_music.csv</th>\n",
       "      <td>m0_</td>\n",
       "      <td>m2_</td>\n",
       "      <td>m2_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/health_insurance.csv</th>\n",
       "      <td>m2_</td>\n",
       "      <td>m2_</td>\n",
       "      <td>m0_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/Moneyball.csv</th>\n",
       "      <td>m0_</td>\n",
       "      <td>m2_</td>\n",
       "      <td>m2_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/solar_flare.csv</th>\n",
       "      <td>m1_</td>\n",
       "      <td>m2_</td>\n",
       "      <td>m1_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/abalone.csv</th>\n",
       "      <td>m1_</td>\n",
       "      <td>m2_</td>\n",
       "      <td>m2_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/cps88wages.csv</th>\n",
       "      <td>m2_</td>\n",
       "      <td>m0_</td>\n",
       "      <td>m0_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/california_housing.csv</th>\n",
       "      <td>m2_</td>\n",
       "      <td>m1_</td>\n",
       "      <td>m0_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/socmob.csv</th>\n",
       "      <td>m2_</td>\n",
       "      <td>m2_</td>\n",
       "      <td>m2_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/white_wine.csv</th>\n",
       "      <td>m0_</td>\n",
       "      <td>m0_</td>\n",
       "      <td>m2_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/train-datasets/energy_efficiency.csv</th>\n",
       "      <td>m3_</td>\n",
       "      <td>m2_</td>\n",
       "      <td>m0_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "fold                                                  0    1    2\n",
       "dataset                                                          \n",
       "data/train-datasets/geographical_origin_of_musi...  m0_  m2_  m2_\n",
       "data/train-datasets/health_insurance.csv            m2_  m2_  m0_\n",
       "data/train-datasets/Moneyball.csv                   m0_  m2_  m2_\n",
       "data/train-datasets/solar_flare.csv                 m1_  m2_  m1_\n",
       "data/train-datasets/abalone.csv                     m1_  m2_  m2_\n",
       "data/train-datasets/cps88wages.csv                  m2_  m0_  m0_\n",
       "data/train-datasets/california_housing.csv          m2_  m1_  m0_\n",
       "data/train-datasets/socmob.csv                      m2_  m2_  m2_\n",
       "data/train-datasets/white_wine.csv                  m0_  m0_  m2_\n",
       "data/train-datasets/energy_efficiency.csv           m3_  m2_  m0_"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_model = pd.read_csv(f'results/trials/{parameters.number}.csv')\n",
    "df_selected_model.pivot(index='dataset', columns='fold', values='model').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m2_    50\n",
       "m0_    39\n",
       "m1_     9\n",
       "m3_     7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_selected_model.pivot(index='dataset', columns='fold', values='model')[0], df_selected_model.pivot(index='dataset', columns='fold', values='model')[1], df_selected_model.pivot(index='dataset', columns='fold', values='model')[2]]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeCorrelationLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NegativeCorrelationLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, labels, ncl_lambda, consensus):\n",
    "        consensus = torch.tensor(consensus, dtype=torch.float)\n",
    "        loss = 0.5 * torch.nn.functional.mse_loss(outputs, labels) \n",
    "        loss -= ncl_lambda * torch.nn.functional.mse_loss(outputs, consensus)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_data_sampling_ensemble(df_parameters, ensemble_name, iterations, k_splits, n_estimators, max_samples=1.0, max_features=1.0, bootstrap=False, dropout_rates=None, snapshot=False, ncl_lambda=None, stacking=False):\n",
    "    logging_info = []\n",
    "    j = -1\n",
    "    for i in tqdm(range(iterations)):\n",
    "        for fname in df_selected_model.dataset.unique():\n",
    "            j += 1\n",
    "            start_timer = time.time()\n",
    "\n",
    "            df = pd.read_csv(fname)\n",
    "            input_size = df.shape[1] - 1\n",
    "\n",
    "            # Divide the data into 3 folds\n",
    "            kf = KFold(n_splits=k_splits, shuffle=True, random_state=42)\n",
    "\n",
    "            # Define the hyperparameters to optimize\n",
    "            fold_id = 0\n",
    "            predictions = []\n",
    "            base_predictions = []\n",
    "            for train_idx, test_idx in kf.split(df):\n",
    "                test = df_selected_model[(df_selected_model.dataset == fname) & (df_selected_model.fold == fold_id)].iloc[0]\n",
    "                bmodel = 'params_' + test.model\n",
    "\n",
    "                df_train_set = df.iloc[train_idx].copy()\n",
    "\n",
    "                ensemble = DataSamplingEnsemble(max_samples=max_samples, max_features=max_features, bootstrap=bootstrap, snapshot=snapshot, stacking=stacking)\n",
    "\n",
    "                loss_function = torch.nn.MSELoss()\n",
    "                if ncl_lambda != None:\n",
    "                    loss_function = NegativeCorrelationLoss()\n",
    "\n",
    "                if snapshot:\n",
    "                    ensemble.add_learner(\n",
    "                        1, \n",
    "                        input_size, \n",
    "                        [parameters[bmodel + 'n_units_l1'], parameters[bmodel + 'n_units_l2']], \n",
    "                        parameters[bmodel + 'activation'],\n",
    "                        loss_function, \n",
    "                        dropout_rates, \n",
    "                        parameters[bmodel + 'lr'],\n",
    "                        ANNEALING,\n",
    "                        ncl_lambda)\n",
    "\n",
    "                    ensemble.setup_data_loaders(df_train_set)\n",
    "                    ensemble.train(epochs=10*n_estimators)\n",
    "\n",
    "                else:\n",
    "                    ensemble.add_learner(\n",
    "                        n_estimators, \n",
    "                        input_size, \n",
    "                        [parameters[bmodel + 'n_units_l1'], parameters[bmodel + 'n_units_l2']], \n",
    "                        parameters[bmodel + 'activation'],\n",
    "                        loss_function, \n",
    "                        dropout_rates, \n",
    "                        parameters[bmodel + 'lr'],\n",
    "                        ANNEALING,\n",
    "                        ncl_lambda)\n",
    "                    \n",
    "                    ensemble.setup_data_loaders(df_train_set)\n",
    "                    ensemble.train(epochs=10)\n",
    "\n",
    "                pred, base_pred = ensemble.predict(df.iloc[test_idx].copy())\n",
    "                predictions.append(pred)\n",
    "                base_predictions.append(base_pred)\n",
    "\n",
    "                fold_id += 1\n",
    "            \n",
    "            df['pred'] = np.nan\n",
    "            for k in range(base_pred.shape[1]):\n",
    "                df[f'base_learner_{k}'] = np.nan\n",
    "                if k % 25 == 0:\n",
    "                    df = df.copy()\n",
    "            for train_idx, test_idx in kf.split(df):\n",
    "                df.loc[test_idx, 'pred'] = predictions.pop(0)\n",
    "                base_predictions_pop = base_predictions.pop(0)\n",
    "                for k in range(base_pred.shape[1]):\n",
    "                    df.loc[test_idx, f'base_learner_{k}'] = base_predictions_pop[f'base_learner_{k}'].values\n",
    "\n",
    "            check_dir(f'results/ensemble/regression/{ensemble_name}')\n",
    "\n",
    "            df[['target', 'pred'] + [f'base_learner_{i}' for i in range(base_pred.shape[1])]].to_csv(f'results/ensemble/regression/{ensemble_name}/{fname.split(\"/\")[-1].split(\".\")[0]}_{i}.csv', index=False)\n",
    "\n",
    "            logging_info.append({'test_id': j, 'name': ensemble_name, 'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features, 'bootstrap': bootstrap, 'dropout_rates': dropout_rates, 'snapshot': snapshot, 'ncl_lambda': ncl_lambda, 'stacking': stacking, 'accuracy': (df['target'] == df['pred']).mean(), 'time': time.time() - start_timer})\n",
    "            pd.DataFrame(logging_info).to_csv(f'results/ensemble/regression/training_logging.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name='single_model',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lv0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Ensembles with 10 Estimators (Lv0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'simple_average-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'bagging-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           bootstrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'random_subspaces-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_features=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'pasting-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_samples=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'dropout-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           dropout_rates=[0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'snapshot-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           snapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'negative_correlation_learning-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           ncl_lambda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'stacking-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           stacking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Ensembles with 25 Estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'bagging-random_subspaces-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           bootstrap=True,\n",
    "                           max_features=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'bagging-pasting-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           bootstrap=True,\n",
    "                           max_samples=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'bagging-dropout-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           bootstrap=True,\n",
    "                           dropout_rates=[0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'bagging-snapshot-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           bootstrap=True,\n",
    "                           snapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'bagging-negative_correlation_learning-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           bootstrap=True,\n",
    "                           ncl_lambda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'bagging-stacking-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           bootstrap=True,\n",
    "                           stacking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'random_subspaces-pasting-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_features=0.7,\n",
    "                           max_samples=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'random_subspaces-dropout-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_features=0.7,\n",
    "                           dropout_rates=[0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'random_subspaces-snapshot-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_features=0.7,\n",
    "                           snapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'random_subspaces-negative_correlation_learning-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_features=0.7,\n",
    "                           ncl_lambda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'random_subspaces-stacking-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_features=0.7,\n",
    "                           stacking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'pasting-dropout-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_samples=0.7,\n",
    "                           dropout_rates=[0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'pasting-snapshot-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_samples=0.7,\n",
    "                           snapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'pasting-negative_correlation_learning-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_samples=0.7,\n",
    "                           ncl_lambda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'pasting-stacking-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           max_samples=0.7,\n",
    "                           stacking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'dropout-snapshot-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           dropout_rates=[0.2, 0.2],\n",
    "                           snapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'dropout-negative_correlation_learning-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           dropout_rates=[0.2, 0.2],\n",
    "                           ncl_lambda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'dropout-stacking-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           dropout_rates=[0.2, 0.2],\n",
    "                           stacking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'snapshot-negative_correlation_learning-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           snapshot=True,\n",
    "                           ncl_lambda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'snapshot-stacking-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           snapshot=True,\n",
    "                           stacking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'negative_correlation_learning-stacking-{N_ESTIMATORS}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=N_ESTIMATORS,\n",
    "                           ncl_lambda=0.1,\n",
    "                           stacking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'dropout-snapshot-{10}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=10,\n",
    "                           dropout_rates=[0.2, 0.2],\n",
    "                           snapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'dropout-snapshot-{100}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=100,\n",
    "                           dropout_rates=[0.2, 0.2],\n",
    "                           snapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_sampling_ensemble(df_parameters,\n",
    "                           ensemble_name=f'dropout-snapshot-{200}',\n",
    "                           iterations=ITERATIONS,\n",
    "                           k_splits=K_SPLITS,\n",
    "                           n_estimators=200,\n",
    "                           dropout_rates=[0.2, 0.2],\n",
    "                           snapshot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
